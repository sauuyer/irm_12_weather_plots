{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54dc3fff",
   "metadata": {},
   "source": [
    "# METBK Intercomparison (Irminger 12)\n",
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd8a78",
   "metadata": {},
   "source": [
    "This data comes from the Revelle (not the Armstrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11aa0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sys.path.append(\"../..\")\n",
    "from utils import Ship, Buoy, md2vect, vect2md, bpr_adjust, rh2q\n",
    "import re\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f3e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cruise start and end dates\n",
    "T1 = pd.to_datetime(\"2025-07-18\")\n",
    "T2 = pd.to_datetime(\"2025-08-07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f91f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platforms\n",
    "ship = Ship()\n",
    "sumo11 = Buoy()\n",
    "sumo12 = Buoy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab818b2",
   "metadata": {},
   "source": [
    "## Ship Data \n",
    "\n",
    "* located on the Raw Data Repo: cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/\n",
    "* The metbk files are organized differently than they are when from the Armstrong - there is a 3 file series per day\n",
    "    1) .DCC files - decoded SAMOS data from the ship's met package\n",
    "    2) .COR files - corrected SAMOS data\n",
    "    3) .LOG files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b882aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/\"\n",
    "DOWNLOAD_DIR = \"ship_files/dcc_files\"\n",
    "OUTPUT_CSV = \"ship_files/merged_dcc.csv\"\n",
    "\n",
    "def get_dcc_file_list(base_url: str):\n",
    "    \"\"\"\n",
    "    Fetch the directory listing at base_url and return a list of .DCC filenames.\n",
    "    Assumes an Apache-style (or similar) directory listing with hrefs.\n",
    "    \"\"\"\n",
    "    resp = requests.get(base_url)\n",
    "    resp.raise_for_status()\n",
    "    html = resp.text\n",
    "\n",
    "    # Find href=\"something.DCC\"\n",
    "    pattern = r'href=\"([^\"]+\\.DCC)\"'\n",
    "    files = re.findall(pattern, html, flags=re.IGNORECASE)\n",
    "\n",
    "    # Deduplicate, preserve order\n",
    "    seen = set()\n",
    "    dcc_files = []\n",
    "    for f in files:\n",
    "        if f not in seen:\n",
    "            seen.add(f)\n",
    "            dcc_files.append(f)\n",
    "    return dcc_files\n",
    "\n",
    "\n",
    "def download_dcc_files(base_url: str, filenames, download_dir: str):\n",
    "    \"\"\"\n",
    "    Download each .DCC file if it doesn't already exist in download_dir.\n",
    "    \"\"\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    for fname in filenames:\n",
    "        local_path = os.path.join(download_dir, fname)\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"Already exists, skipping: {fname}\")\n",
    "            continue\n",
    "\n",
    "        url = base_url + fname\n",
    "        print(f\"Downloading: {url}\")\n",
    "        resp = requests.get(url, stream=True)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"Saved to {local_path}\")\n",
    "\n",
    "\n",
    "def parse_dcc_line(line: str, source_file: str):\n",
    "    \"\"\"\n",
    "    Parse one .DCC line like:\n",
    "    $SAMOS:001,CS:KAOU,YMD:20250716,...\n",
    "\n",
    "    Returns a dict of key -> value including:\n",
    "    - 'record_type' for the first token (e.g., '$SAMOS:001')\n",
    "    - 'source_file' with the original filename\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "    if not line.startswith(\"$\"):\n",
    "        return None\n",
    "\n",
    "    parts = line.split(\",\")\n",
    "    if not parts:\n",
    "        return None\n",
    "\n",
    "    row = {}\n",
    "    # first token: e.g. '$SAMOS:001'\n",
    "    row[\"record_type\"] = parts[0]\n",
    "    row[\"source_file\"] = os.path.basename(source_file)\n",
    "\n",
    "    for token in parts[1:]:\n",
    "        if \":\" not in token:\n",
    "            continue\n",
    "        key, value = token.split(\":\", 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        if key:\n",
    "            row[key] = value\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def merge_dcc_to_csv(download_dir: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Read all .DCC files in download_dir, parse each line,\n",
    "    and write a merged CSV with all unique keys as columns.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    all_keys = set()\n",
    "\n",
    "    # Find all .DCC files\n",
    "    for fname in sorted(os.listdir(download_dir)):\n",
    "        if not fname.upper().endswith(\".DCC\"):\n",
    "            continue\n",
    "        path = os.path.join(download_dir, fname)\n",
    "        print(f\"Parsing {path}\")\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            for line in f:\n",
    "                row = parse_dcc_line(line, source_file=path)\n",
    "                if row:\n",
    "                    all_rows.append(row)\n",
    "                    all_keys.update(row.keys())\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"No data rows parsed; CSV will not be created.\")\n",
    "        return\n",
    "\n",
    "    # Ensure a stable column order: record_type, source_file, then sorted rest\n",
    "    base_cols = [\"record_type\", \"source_file\"]\n",
    "    other_cols = sorted(k for k in all_keys if k not in base_cols)\n",
    "    fieldnames = base_cols + other_cols\n",
    "\n",
    "    print(f\"Writing {len(all_rows)} rows to {output_csv}\")\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as out_f:\n",
    "        writer = csv.DictWriter(out_f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in all_rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214531f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 .DCC files\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250716.DCC\n",
      "Saved to dcc_files/250716.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250717.DCC\n",
      "Saved to dcc_files/250717.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250718.DCC\n",
      "Saved to dcc_files/250718.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250719.DCC\n",
      "Saved to dcc_files/250719.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250720.DCC\n",
      "Saved to dcc_files/250720.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250721.DCC\n",
      "Saved to dcc_files/250721.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250722.DCC\n",
      "Saved to dcc_files/250722.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250723.DCC\n",
      "Saved to dcc_files/250723.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250724.DCC\n",
      "Saved to dcc_files/250724.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250725.DCC\n",
      "Saved to dcc_files/250725.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250726.DCC\n",
      "Saved to dcc_files/250726.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250727.DCC\n",
      "Saved to dcc_files/250727.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250728.DCC\n",
      "Saved to dcc_files/250728.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250729.DCC\n",
      "Saved to dcc_files/250729.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250730.DCC\n",
      "Saved to dcc_files/250730.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250731.DCC\n",
      "Saved to dcc_files/250731.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250801.DCC\n",
      "Saved to dcc_files/250801.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250802.DCC\n",
      "Saved to dcc_files/250802.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250803.DCC\n",
      "Saved to dcc_files/250803.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250804.DCC\n",
      "Saved to dcc_files/250804.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250805.DCC\n",
      "Saved to dcc_files/250805.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250806.DCC\n",
      "Saved to dcc_files/250806.DCC\n",
      "Downloading: https://rawdata.oceanobservatories.org/files/cruise_data/Irminger_Sea/Irminger_Sea-12_RR2505_2025-07-18/Ship_Data/metacq/data/250807.DCC\n",
      "Saved to dcc_files/250807.DCC\n",
      "Parsing dcc_files/250716.DCC\n",
      "Parsing dcc_files/250717.DCC\n",
      "Parsing dcc_files/250718.DCC\n",
      "Parsing dcc_files/250719.DCC\n",
      "Parsing dcc_files/250720.DCC\n",
      "Parsing dcc_files/250721.DCC\n",
      "Parsing dcc_files/250722.DCC\n",
      "Parsing dcc_files/250723.DCC\n",
      "Parsing dcc_files/250724.DCC\n",
      "Parsing dcc_files/250725.DCC\n",
      "Parsing dcc_files/250726.DCC\n",
      "Parsing dcc_files/250727.DCC\n",
      "Parsing dcc_files/250728.DCC\n",
      "Parsing dcc_files/250729.DCC\n",
      "Parsing dcc_files/250730.DCC\n",
      "Parsing dcc_files/250731.DCC\n",
      "Parsing dcc_files/250801.DCC\n",
      "Parsing dcc_files/250802.DCC\n",
      "Parsing dcc_files/250803.DCC\n",
      "Parsing dcc_files/250804.DCC\n",
      "Parsing dcc_files/250805.DCC\n",
      "Parsing dcc_files/250806.DCC\n",
      "Parsing dcc_files/250807.DCC\n",
      "Writing 32202 rows to merged_dcc.csv\n",
      "Done.\n",
      "\n",
      "All done. Merged CSV: merged_dcc.csv\n"
     ]
    }
   ],
   "source": [
    "dcc_files = get_dcc_file_list(BASE_URL)\n",
    "print(f\"Found {len(dcc_files)} .DCC files\")\n",
    "\n",
    "download_dcc_files(BASE_URL, dcc_files, DOWNLOAD_DIR)\n",
    "merge_dcc_to_csv(DOWNLOAD_DIR, OUTPUT_CSV)\n",
    "\n",
    "print(f\"\\nAll done. Merged CSV: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672a9376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validation Report ===\n",
      "\n",
      "File: 250716.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250717.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250718.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250719.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250720.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250721.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250722.DCC\n",
      "  Lines in DCC:       1439\n",
      "  Rows in merged CSV: 1439\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250723.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250724.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250725.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250726.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250727.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250728.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250729.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250730.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250731.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250801.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250802.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250803.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250804.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250805.DCC\n",
      "  Lines in DCC:       1438\n",
      "  Rows in merged CSV: 1438\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250806.DCC\n",
      "  Lines in DCC:       1440\n",
      "  Rows in merged CSV: 1440\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "File: 250807.DCC\n",
      "  Lines in DCC:       525\n",
      "  Rows in merged CSV: 525\n",
      "  ‚úÖ OK ‚Äî all rows accounted for\n",
      "\n",
      "üéâ All DCC files are fully represented in the merged CSV!\n"
     ]
    }
   ],
   "source": [
    "# Validation of the resulting CSV\n",
    "\n",
    "def count_dcc_data_lines(file_path):\n",
    "    \"\"\"\n",
    "    Count lines beginning with '$' in a .DCC file (true data lines).\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"$\"):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def validate_merged_csv(dcc_dir, merged_csv):\n",
    "    df = pd.read_csv(merged_csv)\n",
    "\n",
    "    # Group merged CSV rows by source file\n",
    "    merged_counts = df[\"source_file\"].value_counts().to_dict()\n",
    "\n",
    "    print(\"=== Validation Report ===\\n\")\n",
    "\n",
    "    all_good = True\n",
    "\n",
    "    for fname in sorted(os.listdir(dcc_dir)):\n",
    "        if not fname.upper().endswith(\".DCC\"):\n",
    "            continue\n",
    "\n",
    "        dcc_path = os.path.join(dcc_dir, fname)\n",
    "        dcc_line_count = count_dcc_data_lines(dcc_path)\n",
    "        csv_line_count = merged_counts.get(fname, 0)\n",
    "\n",
    "        print(f\"File: {fname}\")\n",
    "        print(f\"  Lines in DCC:       {dcc_line_count}\")\n",
    "        print(f\"  Rows in merged CSV: {csv_line_count}\")\n",
    "\n",
    "        if dcc_line_count != csv_line_count:\n",
    "            all_good = False\n",
    "            print(\"  ‚ùå MISMATCH ‚Äî some rows may be missing\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ OK ‚Äî all rows accounted for\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    if all_good:\n",
    "        print(\"üéâ All DCC files are fully represented in the merged CSV!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Some files appear incomplete. Investigate mismatches above.\")\n",
    "\n",
    "\n",
    "# Run the validation\n",
    "validate_merged_csv(DOWNLOAD_DIR, OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fda0f9",
   "metadata": {},
   "source": [
    "## Buoy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c134e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metbk_file_list(base_url, sensor_tag, T1, T2):\n",
    "    \"\"\"\n",
    "    Fetch the directory listing at base_url and return a list of\n",
    "    (filename, date) for files like YYYYMMDD.<sensor_tag>.log\n",
    "    within the [T1, T2] date range.\n",
    "\n",
    "    sensor_tag is usually \"metbk1\" or \"metbk2\".\n",
    "    T1, T2 should be datetime.date objects.\n",
    "    \"\"\"\n",
    "    resp = requests.get(base_url)\n",
    "    resp.raise_for_status()\n",
    "    html = resp.text\n",
    "\n",
    "    # href=\"YYYYMMDD.metbkX.log\"\n",
    "    href_pattern = rf'href=\"([^\"]+\\.{sensor_tag}\\.log)\"'\n",
    "    candidates = re.findall(href_pattern, html, flags=re.IGNORECASE)\n",
    "\n",
    "    # Regex to extract date from filename\n",
    "    fname_pattern = re.compile(rf'(\\d{{8}})\\.{sensor_tag}\\.log$', re.IGNORECASE)\n",
    "\n",
    "    selected = []\n",
    "    for fname in candidates:\n",
    "        m = fname_pattern.search(fname)\n",
    "        if not m:\n",
    "            continue\n",
    "        datestr = m.group(1)\n",
    "        file_date = datetime.strptime(datestr, \"%Y%m%d\").date()\n",
    "        if T1 <= file_date <= T2:\n",
    "            selected.append((fname, file_date))\n",
    "\n",
    "    # Deduplicate and sort by date\n",
    "    selected = sorted(set(selected), key=lambda x: x[1])\n",
    "    return selected\n",
    "\n",
    "\n",
    "def download_files(base_url, file_list, download_dir):\n",
    "    \"\"\"\n",
    "    Download each file in file_list if not already in download_dir.\n",
    "    file_list: list of (filename, date)\n",
    "    \"\"\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    for fname, fdate in file_list:\n",
    "        local_path = os.path.join(download_dir, fname)\n",
    "        if os.path.exists(local_path):\n",
    "            print(f\"[skip] {fname} already exists\")\n",
    "            continue\n",
    "\n",
    "        url = base_url + fname\n",
    "        print(f\"[download] {url}\")\n",
    "        resp = requests.get(url, stream=True)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "        print(f\"  -> saved to {local_path}\")\n",
    "\n",
    "\n",
    "def parse_metbk_line(line, source_file):\n",
    "    \"\"\"\n",
    "    Parse a METBK ASIMET non-label mode line:\n",
    "\n",
    "    yyyy/mm/dd HH:MM:SS.sss  BP  RH  RH_T  LWR  PRC  SeaT  Cond  SWR  We  Wn  Bat1  Bat2\n",
    "\n",
    "    Returns a dict with:\n",
    "      datetime, BP, RH, RH_T, LWR, PRC, SeaT, Cond, SWR, We, Wn, Bat1, Bat2, source_file\n",
    "    or None if the line doesn't match expectations.\n",
    "    \"\"\"\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) < 14:\n",
    "        # not enough tokens to be a full record\n",
    "        return None\n",
    "\n",
    "    date_str = parts[0]\n",
    "    time_str = parts[1]\n",
    "    try:\n",
    "        dt = datetime.strptime(date_str + \" \" + time_str, \"%Y/%m/%d %H:%M:%S.%f\")\n",
    "    except ValueError:\n",
    "        # fallback if no milliseconds\n",
    "        try:\n",
    "            dt = datetime.strptime(date_str + \" \" + time_str, \"%Y/%m/%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    # The next 12 values should be numeric fields in the documented order\n",
    "    numeric_vals = parts[2:14]\n",
    "    if len(numeric_vals) != 12:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        (\n",
    "            BP,      # mbar\n",
    "            RH,      # %\n",
    "            RH_T,    # degC\n",
    "            LWR,     # W/m^2\n",
    "            PRC,     # mm\n",
    "            SeaT,    # degC\n",
    "            Cond,    # S/m\n",
    "            SWR,     # W/m^2\n",
    "            We,      # m/s\n",
    "            Wn,      # m/s\n",
    "            Bat1,    # V\n",
    "            Bat2,    # V\n",
    "        ) = map(float, numeric_vals)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"datetime\": dt,\n",
    "        \"BP_mbar\": BP,\n",
    "        \"RH_pct\": RH,\n",
    "        \"RH_T_degC\": RH_T,\n",
    "        \"LWR_W_m2\": LWR,\n",
    "        \"PRC_mm\": PRC,\n",
    "        \"SeaT_degC\": SeaT,\n",
    "        \"Cond_S_m\": Cond,\n",
    "        \"SWR_W_m2\": SWR,\n",
    "        \"We_m_s\": We,\n",
    "        \"Wn_m_s\": Wn,\n",
    "        \"Bat1_V\": Bat1,\n",
    "        \"Bat2_V\": Bat2,\n",
    "        \"source_file\": os.path.basename(source_file),\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_all_metbk_logs(download_dir, sensor_tag):\n",
    "    \"\"\"\n",
    "    Parse all *.{sensor_tag}.log files in download_dir into a single pandas DataFrame.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for fname in sorted(os.listdir(download_dir)):\n",
    "        if not fname.lower().endswith(f\".{sensor_tag}.log\"):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(download_dir, fname)\n",
    "        print(f\"[parse] {path}\")\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            for line in f:\n",
    "                rec = parse_metbk_line(line, source_file=path)\n",
    "                if rec is not None:\n",
    "                    rows.append(rec)\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No valid METBK records parsed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# High-level function you can call for *any* buoy & metbk{1,2}\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def fetch_metbk_range(\n",
    "    base_url,\n",
    "    sensor_tag,\n",
    "    T1,\n",
    "    T2,\n",
    "    download_dir,\n",
    "    output_csv=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch, download, and parse METBK non-label mode logs for a given sensor_tag\n",
    "    (\"metbk1\" or \"metbk2\") between dates T1 and T2 (inclusive).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_url : str\n",
    "        Directory URL where the log files live.\n",
    "    sensor_tag : str\n",
    "        e.g., \"metbk1\" or \"metbk2\".\n",
    "    T1, T2 : pandas.Timestamp or datetime.date\n",
    "        Start and end of date range (inclusive).\n",
    "    download_dir : str\n",
    "        Local directory to store logs.\n",
    "    output_csv : str or None\n",
    "        If provided, save merged DataFrame to this CSV.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        Parsed data for that sensor & date range.\n",
    "    \"\"\"\n",
    "    # Normalize T1/T2 to date objects\n",
    "    T1_date = pd.to_datetime(T1).date()\n",
    "    T2_date = pd.to_datetime(T2).date()\n",
    "\n",
    "    print(f\"\\n=== {sensor_tag} | {T1_date} ‚Üí {T2_date} ===\")\n",
    "\n",
    "    files = get_metbk_file_list(base_url, sensor_tag, T1_date, T2_date)\n",
    "    print(f\"Found {len(files)} {sensor_tag} log files in range:\")\n",
    "    for fname, fdate in files:\n",
    "        print(f\"  {fname}  ({fdate})\")\n",
    "\n",
    "    if not files:\n",
    "        print(\"No matching files; returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    download_files(base_url, files, download_dir)\n",
    "\n",
    "    df = parse_all_metbk_logs(download_dir, sensor_tag)\n",
    "    print(f\"\\nParsed {len(df)} METBK records for {sensor_tag}.\")\n",
    "\n",
    "    if output_csv and not df.empty:\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Saved merged CSV to: {output_csv}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198df57",
   "metadata": {},
   "source": [
    "## Recovered buoy (SUMO-11) metbk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b96f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== metbk1 | 2025-07-18 ‚Üí 2025-08-07 ===\n",
      "Found 3 metbk1 log files in range:\n",
      "  20250730.metbk1.log  (2025-07-30)\n",
      "  20250731.metbk1.log  (2025-07-31)\n",
      "  20250801.metbk1.log  (2025-08-01)\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl11/metbk1/20250730.metbk1.log\n",
      "  -> saved to buoy_files/recovered/metbk1/20250730.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl11/metbk1/20250731.metbk1.log\n",
      "  -> saved to buoy_files/recovered/metbk1/20250731.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl11/metbk1/20250801.metbk1.log\n",
      "  -> saved to buoy_files/recovered/metbk1/20250801.metbk1.log\n",
      "[parse] buoy_files/recovered/metbk1/20250730.metbk1.log\n",
      "[parse] buoy_files/recovered/metbk1/20250731.metbk1.log\n",
      "[parse] buoy_files/recovered/metbk1/20250801.metbk1.log\n",
      "\n",
      "Parsed 3106 METBK records for metbk1.\n",
      "Saved merged CSV to: R_metbk1_20250718_20250807.csv\n",
      "\n",
      "=== metbk2 | 2025-07-18 ‚Üí 2025-08-07 ===\n",
      "Found 15 metbk2 log files in range:\n",
      "  20250718.metbk2.log  (2025-07-18)\n",
      "  20250719.metbk2.log  (2025-07-19)\n",
      "  20250720.metbk2.log  (2025-07-20)\n",
      "  20250721.metbk2.log  (2025-07-21)\n",
      "  20250722.metbk2.log  (2025-07-22)\n",
      "  20250723.metbk2.log  (2025-07-23)\n",
      "  20250724.metbk2.log  (2025-07-24)\n",
      "  20250725.metbk2.log  (2025-07-25)\n",
      "  20250726.metbk2.log  (2025-07-26)\n",
      "  20250727.metbk2.log  (2025-07-27)\n",
      "  20250728.metbk2.log  (2025-07-28)\n",
      "  20250729.metbk2.log  (2025-07-29)\n",
      "  20250730.metbk2.log  (2025-07-30)\n",
      "  20250731.metbk2.log  (2025-07-31)\n",
      "  20250801.metbk2.log  (2025-08-01)\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250718.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250718.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250719.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250719.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250720.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250720.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250721.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250721.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250722.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250722.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250723.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250723.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250724.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250724.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250725.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250725.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250726.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250726.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250727.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250727.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250728.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250728.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250729.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250729.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250730.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250730.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250731.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250731.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/20250801.metbk2.log\n",
      "  -> saved to buoy_files/recovered/metbk2/20250801.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250718.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250719.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250720.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250721.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250722.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250723.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250724.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250725.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250726.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250727.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250728.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250729.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250730.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250731.metbk2.log\n",
      "[parse] buoy_files/recovered/metbk2/20250801.metbk2.log\n",
      "\n",
      "Parsed 17372 METBK records for metbk2.\n",
      "Saved merged CSV to: R_metbk2_20250718_20250807.csv\n"
     ]
    }
   ],
   "source": [
    "R_metbk1_base = \"https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl11/metbk1/\"  \n",
    "R_metbk2_base = \"https://rawdata.oceanobservatories.org/files/GI01SUMO/R00011/cg_data/dcl12/metbk2/\"  \n",
    "\n",
    "df_R_metbk1 = fetch_metbk_range(\n",
    "    base_url=R_metbk1_base,\n",
    "    sensor_tag=\"metbk1\",\n",
    "    T1=T1,\n",
    "    T2=T2,\n",
    "    download_dir=\"buoy_files/recovered/metbk1\",\n",
    "    output_csv=\"R_metbk1_20250718_20250807.csv\",\n",
    ")\n",
    "\n",
    "df_R_metbk2 = fetch_metbk_range(\n",
    "    base_url=R_metbk2_base,\n",
    "    sensor_tag=\"metbk2\",\n",
    "    T1=T1,\n",
    "    T2=T2,\n",
    "    download_dir=\"buoy_files/recovered/metbk2\",\n",
    "    output_csv=\"R_metbk2_20250718_20250807.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32de73b6",
   "metadata": {},
   "source": [
    "## Deployed Buoy (SUMO-12) metbk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba38e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== metbk1 | 2025-07-18 ‚Üí 2025-08-07 ===\n",
      "Found 21 metbk1 log files in range:\n",
      "  20250718.metbk1.log  (2025-07-18)\n",
      "  20250719.metbk1.log  (2025-07-19)\n",
      "  20250720.metbk1.log  (2025-07-20)\n",
      "  20250721.metbk1.log  (2025-07-21)\n",
      "  20250722.metbk1.log  (2025-07-22)\n",
      "  20250723.metbk1.log  (2025-07-23)\n",
      "  20250724.metbk1.log  (2025-07-24)\n",
      "  20250725.metbk1.log  (2025-07-25)\n",
      "  20250726.metbk1.log  (2025-07-26)\n",
      "  20250727.metbk1.log  (2025-07-27)\n",
      "  20250728.metbk1.log  (2025-07-28)\n",
      "  20250729.metbk1.log  (2025-07-29)\n",
      "  20250730.metbk1.log  (2025-07-30)\n",
      "  20250731.metbk1.log  (2025-07-31)\n",
      "  20250801.metbk1.log  (2025-08-01)\n",
      "  20250802.metbk1.log  (2025-08-02)\n",
      "  20250803.metbk1.log  (2025-08-03)\n",
      "  20250804.metbk1.log  (2025-08-04)\n",
      "  20250805.metbk1.log  (2025-08-05)\n",
      "  20250806.metbk1.log  (2025-08-06)\n",
      "  20250807.metbk1.log  (2025-08-07)\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250718.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250718.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250719.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250719.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250720.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250720.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250721.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250721.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250722.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250722.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250723.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250723.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250724.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250724.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250725.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250725.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250726.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250726.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250727.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250727.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250728.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250728.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250729.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250729.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250730.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250730.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250731.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250731.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250801.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250801.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250802.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250802.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250803.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250803.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250804.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250804.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250805.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250805.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250806.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250806.metbk1.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/20250807.metbk1.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250807.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250718.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250719.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250720.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250721.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250722.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250723.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250724.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250725.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250726.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250727.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250728.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250729.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250730.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250731.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250801.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250802.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250803.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250804.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250805.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250806.metbk1.log\n",
      "[parse] buoy_files/deployed/metbk1/20250807.metbk1.log\n",
      "\n",
      "Parsed 30240 METBK records for metbk1.\n",
      "Saved merged CSV to: D_metbk1_20250718_20250807.csv\n",
      "\n",
      "=== metbk2 | 2025-07-18 ‚Üí 2025-08-07 ===\n",
      "Found 21 metbk2 log files in range:\n",
      "  20250718.metbk2.log  (2025-07-18)\n",
      "  20250719.metbk2.log  (2025-07-19)\n",
      "  20250720.metbk2.log  (2025-07-20)\n",
      "  20250721.metbk2.log  (2025-07-21)\n",
      "  20250722.metbk2.log  (2025-07-22)\n",
      "  20250723.metbk2.log  (2025-07-23)\n",
      "  20250724.metbk2.log  (2025-07-24)\n",
      "  20250725.metbk2.log  (2025-07-25)\n",
      "  20250726.metbk2.log  (2025-07-26)\n",
      "  20250727.metbk2.log  (2025-07-27)\n",
      "  20250728.metbk2.log  (2025-07-28)\n",
      "  20250729.metbk2.log  (2025-07-29)\n",
      "  20250730.metbk2.log  (2025-07-30)\n",
      "  20250731.metbk2.log  (2025-07-31)\n",
      "  20250801.metbk2.log  (2025-08-01)\n",
      "  20250802.metbk2.log  (2025-08-02)\n",
      "  20250803.metbk2.log  (2025-08-03)\n",
      "  20250804.metbk2.log  (2025-08-04)\n",
      "  20250805.metbk2.log  (2025-08-05)\n",
      "  20250806.metbk2.log  (2025-08-06)\n",
      "  20250807.metbk2.log  (2025-08-07)\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250718.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250718.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250719.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250719.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250720.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250720.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250721.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250721.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250722.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250722.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250723.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250723.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250724.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250724.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250725.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250725.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250726.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250726.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250727.metbk2.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> saved to buoy_files/deployed/metbk1/20250727.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250728.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250728.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250729.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250729.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250730.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250730.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250731.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250731.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250801.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250801.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250802.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250802.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250803.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250803.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250804.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250804.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250805.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250805.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250806.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250806.metbk2.log\n",
      "[download] https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/20250807.metbk2.log\n",
      "  -> saved to buoy_files/deployed/metbk1/20250807.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250718.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250719.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250720.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250721.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250722.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250723.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250724.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250725.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250726.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250727.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250728.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250729.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250730.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250731.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250801.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250802.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250803.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250804.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250805.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250806.metbk2.log\n",
      "[parse] buoy_files/deployed/metbk1/20250807.metbk2.log\n",
      "\n",
      "Parsed 30241 METBK records for metbk2.\n",
      "Saved merged CSV to: D_metbk2_20250718_20250807.csv\n"
     ]
    }
   ],
   "source": [
    "D_metbk1_base = \"https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl11/metbk1/\"\n",
    "D_metbk2_base = \"https://rawdata.oceanobservatories.org/files/GI01SUMO/D00012/cg_data/dcl12/metbk2/\"\n",
    "\n",
    "df_B_metbk1 = fetch_metbk_range(\n",
    "    base_url=D_metbk1_base,\n",
    "    sensor_tag=\"metbk1\",\n",
    "    T1=T1,\n",
    "    T2=T2,\n",
    "    download_dir=\"buoy_files/deployed/metbk1\",\n",
    "    output_csv=\"D_metbk1_20250718_20250807.csv\",\n",
    ")\n",
    "\n",
    "df_B_metbk2 = fetch_metbk_range(\n",
    "    base_url=D_metbk2_base,\n",
    "    sensor_tag=\"metbk2\",\n",
    "    T1=T1,\n",
    "    T2=T2,\n",
    "    download_dir=\"buoy_files/deployed/metbk1\",\n",
    "    output_csv=\"D_metbk2_20250718_20250807.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
